## 掩码自监督无冻结重新预训练
### 单GPU代码
```
import sys
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW  # 使用PyTorch的AdamW
from transformers import PreTrainedTokenizerFast, get_linear_schedule_with_warmup
from Bio import SeqIO

sys.path.insert(0, 'D:/progen/progen/progen2')
from models.progen.modeling_progen import ProGenForCausalLM
from models.progen.configuration_progen import ProGenConfig

# 加载分词器和模型配置
tokenizer_path = "D:/progen/progen/progen2/tokenizer.json"
tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path, model_max_length=800)
config = ProGenConfig.from_pretrained("D:/progen/progen/progen2/model/progen2-xlarge")
model = ProGenForCausalLM(config)

# 确保分词器有mask_token_id
if tokenizer.mask_token_id is None:
    tokenizer.add_special_tokens({'mask_token': '[MASK]'})

# 定义数据集
class ProteinDataset(torch.utils.data.Dataset):
    def __init__(self, file_path, tokenizer):
        self.tokenizer = tokenizer
        self.sequences = [str(record.seq) for record in SeqIO.parse(file_path, "fasta")]

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        inputs = tokenizer(sequence, return_tensors="pt", truncation=True, max_length=800)
        input_ids = inputs.input_ids.squeeze()
        labels = input_ids.clone()

        # 随机掩码
        rand = torch.rand(input_ids.shape)
        mask_arr = (rand < 0.15) * (input_ids != tokenizer.pad_token_id)
        input_ids[mask_arr] = tokenizer.mask_token_id

        return input_ids, labels

# 数据加载器
dataset = ProteinDataset('my-fasta.fa', tokenizer)
loader = DataLoader(dataset, batch_size=4, shuffle=True)

# 训练配置
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = AdamW(model.parameters(), lr=5e-5)
num_training_steps = 1000  # 示例步数，根据数据量调整
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=num_training_steps)

# 训练循环
model.train()
for epoch in range(3):  # 示例周期数，根据需要调整
    for batch in loader:
        input_ids, labels = [x.to(device) for x in batch]
        outputs = model(input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
        print(f"Loss: {loss.item()}")

# 保存模型
model_save_path = './new-model-xlarge'
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)
```
